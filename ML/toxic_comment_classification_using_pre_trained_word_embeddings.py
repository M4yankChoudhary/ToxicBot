# -*- coding: utf-8 -*-
"""Toxic Comment Classification using Pre-Trained Word Embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N6y43z2ioQp0fMYrRlDfnrLdtSQIDQk0

# Import the Dataset
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras import Sequential
from keras.layers import Dense, Embedding, GlobalMaxPool1D, LSTM
from keras.losses import BinaryCrossentropy
from keras.metrics import AUC
from keras.optimizers import Adam
from keras.models import model_from_json
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

import re
import gc
import pickle

print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
tf.debugging.set_log_device_placement(True)

! pip install kaggle

from google.colab import files
files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle competitions download -c jigsaw-toxic-comment-classification-challenge

! mkdir dataset

! unzip test.csv.zip -d dataset

! unzip train.csv.zip -d dataset

"""# Download GloVe Word Embeddings"""

! wget http://nlp.stanford.edu/data/glove.840B.300d.zip

! unzip glove.840B.300d.zip

"""# Data Fetching"""

train = pd.read_csv('dataset/train.csv', dtype={'comment_text':'string'})
train.head()

train = train.drop(columns='id')
train.head()

test = pd.read_csv('dataset/test.csv', dtype={'comment_text':'string'})
ids = test.iloc[:,0]
test = test.drop(columns='id')
test.head()

ids.head()

"""# Preprocessing"""

X = train['comment_text'].values
Y = train.iloc[:,1:].values

print(X.shape)

print(Y.shape)
Y

X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)

tokenizer = Tokenizer()

tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

len(X_train_seq)

print(len(tokenizer.word_index))

len(X_test)

X_train_seq = pad_sequences(X_train_seq, maxlen=250)
X_test_seq = pad_sequences(X_test_seq, maxlen=250)

X_test_seq.shape

X_train_seq.shape

"""# Pre-Trained Embedding"""

vocab_size = len(tokenizer.word_index) + 1
vocab_size

embeddings_index = dict()
glove = open('glove.840B.300d.txt')

for line in glove:
    word, coefs = line.split(maxsplit=1)
    coefs = np.fromstring(coefs, "f", sep=" ")
    embeddings_index[word] = coefs

print("Found %s word vectors." % len(embeddings_index))

glove.close()

embedding_matrix = np.zeros((vocab_size, 300))
miss = 0

for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
      if embedding_vector.shape[0] != 0:
        embedding_matrix[i] = embedding_vector
      else:
        miss+=1

print(miss)

embedding_matrix.shape

model = Sequential()

model.add(Embedding(input_dim=vocab_size, output_dim = 300, input_length = 250, weights=[embedding_matrix], trainable = False))

model.add(LSTM(units=150,return_sequences=True, dropout=0.1))

model.add(GlobalMaxPool1D())

model.add(Dense(units = 64, activation='relu'))

model.add(Dense(units = 16, activation='relu'))

model.add(Dense(units = 6, activation='sigmoid'))

model.compile(loss=BinaryCrossentropy(),optimizer=Adam(),metrics=[AUC()])

print(model.summary())

history = model.fit(np.array(X_train_seq), np.array(y_train), batch_size=256, epochs=10, validation_data=(np.array(X_test_seq),np.array(y_test)))

model_json = model.to_json()

with open('glove_embedding.json', 'w') as json_file:
  json_file.write(model_json)

model.save_weights("weights.h5")

json_file = open('glove_embedding.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)

with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

loaded_model.load_weights("weights.h5")

loaded_model.compile(loss=BinaryCrossentropy(),optimizer=Adam(),metrics=[AUC()])

"""# Kaggle Submission"""

test.head()

test_X = test['comment_text'].values
test_X

test_X_seq = tokenizer.texts_to_sequences(test_X)

test_X_seq = pad_sequences(test_X_seq, maxlen=250)

prediction = loaded_model.predict(test_X_seq)
prediction

prediction.shape

result = pd.DataFrame()
result.head()

result["id"] = ids
result.head()

result["toxic"] = prediction[:,0]
result["severe_toxic"] = prediction[:,1]
result["obscene"] = prediction[:,2]
result["threat"] = prediction[:,3]
result["insult"] = prediction[:,4]
result["identity_hate"] = prediction[:,5]
result.head()

result.to_csv('submission.csv', index=False)

! kaggle competitions submit -c jigsaw-toxic-comment-classification-challenge -f submission.csv -m "Using GloVe Word Embeddings and LSTM instead of CUDNNLSTM"