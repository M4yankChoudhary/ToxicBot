# -*- coding: utf-8 -*-
"""Toxic Comment Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dRvXLOSmEwfRRIctLiTROt4-UVGxbXtk

# Toxic Comment Classification
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import roc_auc_score
from scipy.sparse import hstack
import nltk
import spacy 

import re
import string
import gc
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

nltk.download('wordnet')

!python -m spacy download en_core_web_lg

!python -m spacy link en_core_web_lg en --force

import en_core_web_lg

df = pd.read_csv('train.csv')
df.head()

df.isna().sum()

df = df.drop(columns='id')
df.head()

categories = df.columns[1:]
categories

"""# Preprocessing"""

def remove_abbreviation(text):
    text = re.sub("\n"," ",text)
    text = re.sub("\[.*\]"," ",text)
    text = re.sub("\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}"," ",text)
    text = re.sub(r"\?"," ",text)
    text = re.sub("don't","do not",text)
    text = re.sub("doesn't", "does not",text)
    text = re.sub("didn't", "did not",text)
    text = re.sub("hasn't", "has not",text)
    text = re.sub("haven't", "have not",text)
    text = re.sub("hadn't", "had not",text)
    text = re.sub("won't", "will not",text)
    text = re.sub("wouldn't", "would not",text)
    text = re.sub("can't", "can not",text)
    text = re.sub("cannot", "can not",text)
    text = re.sub("i'm", "i am",text)
    text = re.sub("i'll", "i will",text)
    text = re.sub("its", "it is",text)
    text = re.sub("it's", "it is",text)
    text = re.sub("that's", "that is",text)
    text = re.sub("weren't", "were not",text)
    text = re.sub("i'd","i would",text)
    text = re.sub("i've","i have",text)
    text = re.sub("she'd","she would",text)
    text = re.sub("they'll","they will",text)
    text = re.sub("they're","they are",text)
    text = re.sub("we'd","we would",text)
    text = re.sub("we'll","we will",text)
    text = re.sub("we've","we have",text)
    text = re.sub("it'll","it will",text)
    text = re.sub("there's","there is",text)
    text = re.sub("where's","where is",text)
    text = re.sub("they're","they are",text)
    text = re.sub("let's","let us",text)
    text = re.sub("couldn't","could not",text)
    text = re.sub("shouldn't","should not",text)
    text = re.sub("wasn't","was not",text)
    text = re.sub("could've","could have",text)
    text = re.sub("might've","might have",text)
    text = re.sub("must've","must have",text)
    text = re.sub("should've","should have",text)
    text = re.sub("would've","would have",text)
    text = re.sub("who's","who is",text)
    text = re.sub("\bim\b", "i am",text)
    text = re.sub(r'[^\w\s]','',text)
    text = re.sub("\d+", "", text)
    return text

def to_lower(text):
  return text.lower()

PUNCT_TO_REMOVE = string.punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))

from  spacy.lang.en.stop_words import STOP_WORDS
stopwords = list(STOP_WORDS)

def remove_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in stopwords])

nlp = spacy.load('en')
  
def lemmatize(text):
  doc = nlp(text)
  tokens = [] 
  for token in doc: 
      tokens.append(token)
  return " ".join([token.lemma_ for token in doc])

def preprocessing_pipeline(text):
  text = remove_abbreviation(text)
  text = to_lower(text)
  text = remove_punctuation(text)
  text = remove_stopwords(text)
  text = lemmatize(text)
  return text

df['comment_text'] = df.loc[:,'comment_text'].apply(lambda text : preprocessing_pipeline(text))

df.to_csv('processed.csv', index=False)

del remove_abbreviation
del to_lower
del remove_punctuation
del remove_stopwords
del lemmatize
del preprocessing_pipeline

del PUNCT_TO_REMOVE
del stopwords
del nlp

gc.collect()

"""# Text to Numeric Conversion"""

X = df.iloc[:,0].values.astype(str)
X

Y = df.iloc[:,1:].values
Y.shape

del df
gc.collect()

X_train, X_test, y_train, y_test = train_test_split(X,Y)

word_vectorizer = TfidfVectorizer(
    sublinear_tf=True,
    strip_accents='unicode',
    analyzer='word',
    token_pattern=r'\w{1,}',
    stop_words='english',
    ngram_range=(1, 2),
    max_features=10000,
    dtype=np.float32
)

X_train_word = word_vectorizer.fit_transform(X_train)

X_test_word = word_vectorizer.transform(X_test)

X_train_word

char_vectorizer = TfidfVectorizer(
    sublinear_tf=True,
    strip_accents='unicode',
    analyzer='char',
    ngram_range=(2, 6),
    max_features=20000,
    dtype=np.float32)

X_train_char = char_vectorizer.fit_transform(X_train)

X_test_char = char_vectorizer.transform(X_test)

X_train_char

X_train = hstack([X_train_word, X_train_char])
X_test = hstack([X_test_word, X_test_char])

del X_train_char
del X_test_char
del X_test_word
del X_train_word

del X
del Y

del char_vectorizer
del word_vectorizer

gc.collect()

"""# Logistic Regression"""

scores = 0
for index,category in enumerate(categories):
    logistic_regression = LogisticRegression(C=0.1, solver='sag')
    score = np.mean(cross_val_score(logistic_regression, X_train, y_train[:,index], cv=3, scoring='roc_auc'))
    scores += score
    print(f"{category} : {score}")

print("\nAverage Score : {:.3f}".format(scores/6))